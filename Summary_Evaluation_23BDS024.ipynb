{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1707406d",
      "metadata": {
        "id": "1707406d"
      },
      "source": [
        "# AI Summary Evaluator\n",
        "## Automatic Grading System for Student Lecture Summaries\n",
        "\n",
        "**Author:** J Ganesh Kumar Reddy 23bds024  \n",
        "**Purpose:** Evaluate student-written summaries against lecture transcript using AI  \n",
        "**Date:** 2025-11-13\n",
        "\n",
        "---\n",
        "\n",
        "### System Overview:\n",
        "This notebook implements an automated evaluation system that:\n",
        "1. Loads lecture transcript and student summaries\n",
        "2. Uses Sentence Transformers for semantic similarity analysis\n",
        "3. Evaluates summaries on multiple parameters (Coverage, Relevance, Clarity, Coherence, Grammar)\n",
        "4. Generates scores out of 10 with detailed explanations\n",
        "5. Exports results to Excel automatically"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a43024a4",
      "metadata": {
        "id": "a43024a4"
      },
      "source": [
        "## Step 1: Import Required Libraries\n",
        "Installing and importing all necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "40e57227",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40e57227",
        "outputId": "d4c51d2a-a02d-4ff4-af5f-00be1c0472eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing required packages...\n",
            "  Installing pandas...\n",
            "  Installing numpy...\n",
            "  Installing openpyxl...\n",
            "  Installing python-docx...\n",
            "  Installing sentence-transformers...\n",
            "  Installing nltk...\n",
            "  Installing textstat...\n",
            "  Installing scikit-learn...\n",
            "  Installing torch...\n",
            "âœ“ All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run once)\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "packages = [\n",
        "    'pandas',\n",
        "    'numpy',\n",
        "    'openpyxl',\n",
        "    'python-docx',\n",
        "    'sentence-transformers',\n",
        "    'nltk',\n",
        "    'textstat',\n",
        "    'scikit-learn',\n",
        "    'torch'\n",
        "]\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "for package in packages:\n",
        "    print(f\"  Installing {package}...\")\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', package])\n",
        "\n",
        "print(\"âœ“ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "303fb68c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "303fb68c",
        "outputId": "08b2090b-3b41-46cd-e9ef-455b92acd6c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\RAVIPRAKASH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from docx import Document\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import textstat\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "print(\"âœ“ All libraries imported successfully!\")\n",
        "print(f\"âœ“ Current working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5231eaf",
      "metadata": {
        "id": "e5231eaf"
      },
      "source": [
        "## Step 2: Load AI Model\n",
        "Loading the Sentence Transformer model for semantic similarity analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2244e2aa",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "d78d9b048ec9477cb4858135e2a7c66a",
            "f4bcb3e822de4fc3ae1d4a94b650453e",
            "8de88fee68394152a8c9690a9b1f065f",
            "43d8546dc62c4e98aa2b5d8bb9e79cc1",
            "c178810d3f984078a5ff217eb2c991e7",
            "c5bbdc2b27874cbf831698e54d3f10fb",
            "a27fc69c6f0b4edeba7eac0f2fe477ed",
            "c0e435c096e04746b2a882d288fc3f4b",
            "cbf6d7e3632a4d63af3e84cafe462bc5",
            "b33f95243f7247a19f4e0434be2f8a81",
            "26520929d91a4835a47b73242c2c7388"
          ]
        },
        "id": "2244e2aa",
        "outputId": "571ac472-389a-4404-9202-4b6e0508bbd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Sentence Transformer model...\n",
            "âœ“ Model loaded successfully!\n",
            "  Model: paraphrase-MiniLM-L6-v2\n",
            "  Embedding dimension: 384\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained Sentence Transformer model\n",
        "print(\"Loading Sentence Transformer model...\")\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "print(\"âœ“ Model loaded successfully!\")\n",
        "print(f\"  Model: paraphrase-MiniLM-L6-v2\")\n",
        "print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de0a48dc",
      "metadata": {
        "id": "de0a48dc"
      },
      "source": [
        "## Step 3: Load Input Files\n",
        "Reading lecture transcript and student summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed06de9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9ed06de9",
        "outputId": "9c9e23c3-07bf-4df9-e4e4-705bd32733b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Setting up NLTK ===\n",
            "\n",
            "=== Loading Input Files ===\n",
            "âœ“ Transcript loaded: 61243 characters\n",
            "âœ“ Summaries loaded: 82 students\n",
            "  Columns: ['Timestamp', 'Email Address', 'Name of the Student', 'Roll number', 'Institute', 'Summary(in 100words)']\n",
            "\n",
            "âœ“ All files loaded successfully!\n",
            "\n",
            "Preview of loaded data:\n",
            "                Timestamp           Email Address     Name of the Student  \\\n",
            "0 2025-08-07 19:57:08.844   cs22b1027@iiitr.ac.in                 Deepthi   \n",
            "1 2025-08-07 19:57:23.709  23bds014@iiitdwd.ac.in            Bongu Ashish   \n",
            "2 2025-08-07 19:58:19.742   cs22b1036@iiitr.ac.in  Mudavath Srinivas Naik   \n",
            "\n",
            "  Roll number     Institute                               Summary(in 100words)  \n",
            "0   Cs22b1027  IIIT Raichur  Learned about neural networks in AI generative...  \n",
            "1    23bds014  IIIT Dharwad                                         Insightful  \n",
            "2   Cs22b1036  IIIT Raichur  In todayâ€™s class, I learned about the Med Agen...  \n"
          ]
        }
      ],
      "source": [
        "def load_transcript(file_path):\n",
        "    \"\"\"\n",
        "    Load lecture transcript from DOCX file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        transcript = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()])\n",
        "        print(f\"âœ“ Transcript loaded: {len(transcript)} characters\")\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error loading transcript: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_summaries(file_path):\n",
        "    \"\"\"\n",
        "    Load student summaries from Excel file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        print(f\"âœ“ Summaries loaded: {len(df)} students\")\n",
        "        print(f\"  Columns: {list(df.columns)}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Error loading summaries: {e}\")\n",
        "        return None\n",
        "\n",
        "# Download NLTK data with proper error handling\n",
        "print(\"\\n=== Setting up NLTK ===\")\n",
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)  # New tokenizer format\n",
        "\n",
        "# Load files\n",
        "print(\"\\n=== Loading Input Files ===\")\n",
        "transcript = load_transcript('transcript.docx')\n",
        "summaries_df = load_summaries('summary.xlsx')\n",
        "\n",
        "if transcript and summaries_df is not None:\n",
        "    print(\"\\nâœ“ All files loaded successfully!\")\n",
        "    print(f\"\\nPreview of loaded data:\")\n",
        "    print(summaries_df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1097cc4",
      "metadata": {
        "id": "f1097cc4"
      },
      "source": [
        "## Step 4: Define Evaluation Functions\n",
        "Implementing multiple evaluation criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12ec5b5",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d12ec5b5",
        "outputId": "1323aa2b-319f-4226-a321-0cee16eaacc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Evaluation functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def extract_key_sentences(text, top_n=10):\n",
        "    \"\"\"\n",
        "    Extract key sentences from text using TF-IDF\n",
        "\n",
        "    Args:\n",
        "        text: Input text\n",
        "        top_n: Number of key sentences to extract\n",
        "\n",
        "    Returns:\n",
        "        list: Key sentences\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    if len(sentences) <= top_n:\n",
        "        return sentences\n",
        "\n",
        "    # Use TF-IDF to find important sentences\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "    scores = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
        "    top_indices = scores.argsort()[-top_n:][::-1]\n",
        "\n",
        "    return [sentences[i] for i in sorted(top_indices)]\n",
        "\n",
        "def evaluate_coverage(summary, transcript, model):\n",
        "    \"\"\"\n",
        "    Evaluate how much important content from transcript is captured\n",
        "    Uses semantic similarity with key sentences\n",
        "\n",
        "    Returns:\n",
        "        float: Score 0-2\n",
        "    \"\"\"\n",
        "    # Extract key sentences from transcript\n",
        "    key_sentences = extract_key_sentences(transcript, top_n=15)\n",
        "\n",
        "    # Encode summary and key sentences\n",
        "    summary_embedding = model.encode(summary, convert_to_tensor=True)\n",
        "    key_embeddings = model.encode(key_sentences, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = util.cos_sim(summary_embedding, key_embeddings)[0]\n",
        "\n",
        "    # Score based on coverage of key points\n",
        "    high_similarity_count = (similarities > 0.4).sum().item()\n",
        "    coverage_ratio = high_similarity_count / len(key_sentences)\n",
        "\n",
        "    return min(2.0, coverage_ratio * 2.5)\n",
        "\n",
        "def evaluate_relevance(summary, transcript, model):\n",
        "    \"\"\"\n",
        "    Evaluate how relevant the summary is to the transcript\n",
        "    Uses overall semantic similarity\n",
        "\n",
        "    Returns:\n",
        "        float: Score 0-2\n",
        "    \"\"\"\n",
        "    # Encode both texts\n",
        "    summary_embedding = model.encode(summary, convert_to_tensor=True)\n",
        "    transcript_embedding = model.encode(transcript[:5000], convert_to_tensor=True)  # Limit for efficiency\n",
        "\n",
        "    # Calculate semantic similarity\n",
        "    similarity = util.cos_sim(summary_embedding, transcript_embedding)[0][0].item()\n",
        "\n",
        "    # Convert to score\n",
        "    return min(2.0, similarity * 2.5)\n",
        "\n",
        "def evaluate_clarity(summary):\n",
        "    \"\"\"\n",
        "    Evaluate readability and flow of the summary\n",
        "    Uses Flesch Reading Ease score\n",
        "\n",
        "    Returns:\n",
        "        float: Score 0-2\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Flesch Reading Ease (higher = easier to read)\n",
        "        reading_ease = textstat.flesch_reading_ease(summary)\n",
        "\n",
        "        # Convert to 0-2 scale (60-100 is good readability)\n",
        "        if reading_ease >= 60:\n",
        "            return 2.0\n",
        "        elif reading_ease >= 40:\n",
        "            return 1.5\n",
        "        elif reading_ease >= 20:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.5\n",
        "    except:\n",
        "        return 1.0  # Default if calculation fails\n",
        "\n",
        "def evaluate_coherence(summary):\n",
        "    \"\"\"\n",
        "    Evaluate logical structure and flow\n",
        "    Checks sentence count, transitions, and structure\n",
        "\n",
        "    Returns:\n",
        "        float: Score 0-2\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(summary)\n",
        "    score = 0.0\n",
        "\n",
        "    # Check sentence count (3-8 is good for 100 word summary)\n",
        "    if 3 <= len(sentences) <= 8:\n",
        "        score += 1.0\n",
        "    elif len(sentences) >= 2:\n",
        "        score += 0.5\n",
        "\n",
        "    # Check for transition words\n",
        "    transition_words = ['however', 'therefore', 'additionally', 'furthermore', 'moreover',\n",
        "                       'consequently', 'thus', 'also', 'first', 'second', 'finally', 'next']\n",
        "    summary_lower = summary.lower()\n",
        "    transitions_found = sum(1 for word in transition_words if word in summary_lower)\n",
        "\n",
        "    if transitions_found >= 2:\n",
        "        score += 1.0\n",
        "    elif transitions_found >= 1:\n",
        "        score += 0.5\n",
        "\n",
        "    return min(2.0, score)\n",
        "\n",
        "def evaluate_grammar(summary):\n",
        "    \"\"\"\n",
        "    Evaluate grammar and language correctness\n",
        "    Basic checks for common issues\n",
        "\n",
        "    Returns:\n",
        "        float: Score 0-2\n",
        "    \"\"\"\n",
        "    score = 2.0\n",
        "\n",
        "    # Check for multiple spaces\n",
        "    if re.search(r'\\s{2,}', summary):\n",
        "        score -= 0.2\n",
        "\n",
        "    # Check for proper sentence capitalization\n",
        "    sentences = sent_tokenize(summary)\n",
        "    uncapitalized = sum(1 for s in sentences if s and not s[0].isupper())\n",
        "    score -= uncapitalized * 0.2\n",
        "\n",
        "    # Check for sentence ending punctuation\n",
        "    no_ending = sum(1 for s in sentences if s and s[-1] not in '.!?')\n",
        "    score -= no_ending * 0.2\n",
        "\n",
        "    # Check spelling/grammar quality using textstat\n",
        "    try:\n",
        "        avg_word_length = textstat.avg_character_per_word(summary)\n",
        "        if avg_word_length < 3:  # Very short words might indicate issues\n",
        "            score -= 0.3\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return max(0.5, min(2.0, score))\n",
        "\n",
        "print(\"âœ“ Evaluation functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89db0e03",
      "metadata": {
        "id": "89db0e03"
      },
      "source": [
        "## Step 5: Main Evaluation Function\n",
        "Combining all criteria and generating explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "123e04cc",
      "metadata": {
        "id": "123e04cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Main evaluation function ready!\n"
          ]
        }
      ],
      "source": [
        "def evaluate_summary(summary, transcript, model):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a student summary\n",
        "\n",
        "    Args:\n",
        "        summary: Student's summary text\n",
        "        transcript: Original lecture transcript\n",
        "        model: Sentence Transformer model\n",
        "\n",
        "    Returns:\n",
        "        dict: Score and detailed evaluation\n",
        "    \"\"\"\n",
        "    # Handle missing or invalid summaries\n",
        "    if not summary or not isinstance(summary, str) or len(summary.strip()) < 20:\n",
        "        return {\n",
        "            'score': 0.0,\n",
        "            'coverage': 0.0,\n",
        "            'relevance': 0.0,\n",
        "            'clarity': 0.0,\n",
        "            'coherence': 0.0,\n",
        "            'grammar': 0.0,\n",
        "            'explanation': 'Invalid or missing summary provided.'\n",
        "        }\n",
        "\n",
        "    # Evaluate on each criterion\n",
        "    coverage_score = evaluate_coverage(summary, transcript, model)\n",
        "    relevance_score = evaluate_relevance(summary, transcript, model)\n",
        "    clarity_score = evaluate_clarity(summary)\n",
        "    coherence_score = evaluate_coherence(summary)\n",
        "    grammar_score = evaluate_grammar(summary)\n",
        "\n",
        "    # Calculate total score (each criterion worth 2 points, total 10)\n",
        "    total_score = coverage_score + relevance_score + clarity_score + coherence_score + grammar_score\n",
        "\n",
        "    # Generate explanation\n",
        "    explanation_parts = []\n",
        "\n",
        "    # Coverage feedback\n",
        "    if coverage_score >= 1.5:\n",
        "        explanation_parts.append(\"Excellent coverage of key lecture points\")\n",
        "    elif coverage_score >= 1.0:\n",
        "        explanation_parts.append(\"Good coverage but misses some important details\")\n",
        "    else:\n",
        "        explanation_parts.append(\"Limited coverage of key lecture content\")\n",
        "\n",
        "    # Relevance feedback\n",
        "    if relevance_score >= 1.5:\n",
        "        explanation_parts.append(\"highly relevant to transcript\")\n",
        "    elif relevance_score >= 1.0:\n",
        "        explanation_parts.append(\"mostly relevant with some tangential content\")\n",
        "    else:\n",
        "        explanation_parts.append(\"lacks relevance to main lecture themes\")\n",
        "\n",
        "    # Writing quality feedback\n",
        "    if clarity_score >= 1.5 and grammar_score >= 1.5:\n",
        "        explanation_parts.append(\"Well-written with clear language and good grammar\")\n",
        "    elif clarity_score >= 1.0 or grammar_score >= 1.0:\n",
        "        explanation_parts.append(\"Adequate writing quality with minor issues\")\n",
        "    else:\n",
        "        explanation_parts.append(\"Needs improvement in clarity and grammar\")\n",
        "\n",
        "    # Coherence feedback\n",
        "    if coherence_score < 1.0:\n",
        "        explanation_parts.append(\"Could improve logical flow and structure\")\n",
        "\n",
        "    explanation = \". \".join(explanation_parts) + \".\"\n",
        "\n",
        "    return {\n",
        "        'score': round(total_score, 2),\n",
        "        'coverage': round(coverage_score, 2),\n",
        "        'relevance': round(relevance_score, 2),\n",
        "        'clarity': round(clarity_score, 2),\n",
        "        'coherence': round(coherence_score, 2),\n",
        "        'grammar': round(grammar_score, 2),\n",
        "        'explanation': explanation\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Main evaluation function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "661a2446",
      "metadata": {
        "id": "661a2446"
      },
      "source": [
        "## Step 6: Batch Evaluation Process\n",
        "Evaluating all student summaries with progress tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad51cf5",
      "metadata": {
        "id": "3ad51cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting evaluation of 82 student summaries...\n",
            "============================================================\n",
            "\n",
            "[1/82] Evaluating: Deepthi\n",
            "  â†’ Score: 2.85/10\n",
            "\n",
            "[2/82] Evaluating: Bongu Ashish\n",
            "  â†’ Score: 0.0/10\n",
            "\n",
            "[3/82] Evaluating: Mudavath Srinivas Naik\n",
            "  â†’ Score: 5.01/10\n",
            "\n",
            "[4/82] Evaluating: Aman Chaurasia\n",
            "  â†’ Score: 5.09/10\n",
            "\n",
            "[5/82] Evaluating: Krishu Patel\n",
            "  â†’ Score: 4.61/10\n",
            "\n",
            "[6/82] Evaluating: Mohammed Arsalan \n",
            "  â†’ Score: 4.25/10\n",
            "\n",
            "[7/82] Evaluating: Mohammed Nabeel Ahsan\n",
            "  â†’ Score: 3.74/10\n",
            "\n",
            "[8/82] Evaluating: MODINE Karthik\n",
            "  â†’ Score: 5.76/10\n",
            "\n",
            "[9/82] Evaluating: Chidvilasini Sridharala \n",
            "  â†’ Score: 4.86/10\n",
            "\n",
            "[10/82] Evaluating: M Jagadeeswar Reddy\n",
            "  â†’ Score: 6.59/10\n",
            "\n",
            "[11/82] Evaluating: Jyoti Gadad\n",
            "  â†’ Score: 5.88/10\n",
            "\n",
            "[12/82] Evaluating: Prasad K\n",
            "  â†’ Score: 4.51/10\n",
            "\n",
            "[13/82] Evaluating: Pavan Kumar\n",
            "  â†’ Score: 5.38/10\n",
            "\n",
            "[14/82] Evaluating: Aditya Gupta \n",
            "  â†’ Score: 6.17/10\n",
            "\n",
            "[15/82] Evaluating: Ritik Kumar Shahi \n",
            "  â†’ Score: 5.42/10\n",
            "\n",
            "[16/82] Evaluating: Pratyush Kumar \n",
            "  â†’ Score: 5.72/10\n",
            "\n",
            "[17/82] Evaluating: A.M.D.Pradeep\n",
            "  â†’ Score: 4.63/10\n",
            "\n",
            "[18/82] Evaluating: Shaik Sajid \n",
            "  â†’ Score: 3.97/10\n",
            "\n",
            "[19/82] Evaluating: AKSHAY KUMAR \n",
            "  â†’ Score: 5.31/10\n",
            "\n",
            "[20/82] Evaluating: Debojyoti Roy\n",
            "  â†’ Score: 5.23/10\n",
            "\n",
            "[21/82] Evaluating: Suhaas S \n",
            "  â†’ Score: 4.92/10\n",
            "\n",
            "[22/82] Evaluating: Usha naga sri \n",
            "  â†’ Score: 2.87/10\n",
            "\n",
            "[23/82] Evaluating: Swastik R\n",
            "  â†’ Score: 6.35/10\n",
            "\n",
            "[24/82] Evaluating: Kunal Kumar Shae\n",
            "  â†’ Score: 5.44/10\n",
            "\n",
            "[25/82] Evaluating: Hitik Adwani\n",
            "  â†’ Score: 5.7/10\n",
            "\n",
            "[26/82] Evaluating: Shruthi BS\n",
            "  â†’ Score: 5.62/10\n",
            "\n",
            "[27/82] Evaluating: J Ganesh Kumar Reddy\n",
            "  â†’ Score: 6.04/10\n",
            "\n",
            "[28/82] Evaluating: Varshith \n",
            "  â†’ Score: 5.88/10\n",
            "\n",
            "[29/82] Evaluating: DUDLEWAR ADINATH BALAJIRAO\n",
            "  â†’ Score: 6.15/10\n",
            "\n",
            "[30/82] Evaluating: Rahul Soni\n",
            "  â†’ Score: 5.75/10\n",
            "\n",
            "[31/82] Evaluating: ANNEM VENKATA KISHAN KUMAR REDDY \n",
            "  â†’ Score: 5.81/10\n",
            "\n",
            "[32/82] Evaluating: G Rohith Yadav\n",
            "  â†’ Score: 5.23/10\n",
            "\n",
            "[33/82] Evaluating: Deependra Kumar Singh\n",
            "  â†’ Score: 6.03/10\n",
            "\n",
            "[34/82] Evaluating: Nirbhay Gupta\n",
            "  â†’ Score: 5.76/10\n",
            "\n",
            "[35/82] Evaluating: Shamanth Rao \n",
            "  â†’ Score: 5.55/10\n",
            "\n",
            "[36/82] Evaluating: Prachit Deshinge\n",
            "  â†’ Score: 5.3/10\n",
            "\n",
            "[37/82] Evaluating: Navya sree ram kumar chowdary \n",
            "  â†’ Score: 6.05/10\n",
            "\n",
            "[38/82] Evaluating: Nachiket Ganesh Apte\n",
            "  â†’ Score: 5.77/10\n",
            "\n",
            "[39/82] Evaluating: Manikesh Kumar\n",
            "  â†’ Score: 4.04/10\n",
            "\n",
            "[40/82] Evaluating: Deepak sharma\n",
            "  â†’ Score: 6.57/10\n",
            "\n",
            "[41/82] Evaluating: Viraj Surana\n",
            "  â†’ Score: 5.57/10\n",
            "\n",
            "[42/82] Evaluating: Nelabhotla Anand Siva Ram\n",
            "  â†’ Score: 4.63/10\n",
            "\n",
            "[43/82] Evaluating: Deva anand M\n",
            "  â†’ Score: 6.21/10\n",
            "\n",
            "[44/82] Evaluating: Aryan Anilkumar Talikoti \n",
            "  â†’ Score: 7.09/10\n",
            "\n",
            "[45/82] Evaluating: Vimal N B\n",
            "  â†’ Score: 4.17/10\n",
            "\n",
            "[46/82] Evaluating: Ankit Kushwaha\n",
            "  â†’ Score: 4.54/10\n",
            "\n",
            "[47/82] Evaluating: Amritanshu Aditya\n",
            "  â†’ Score: 6.44/10\n",
            "\n",
            "[48/82] Evaluating: Meghana kommana\n",
            "  â†’ Score: 5.91/10\n",
            "\n",
            "[49/82] Evaluating: Ashutosh Singh\n",
            "  â†’ Score: 6.15/10\n",
            "\n",
            "[50/82] Evaluating: Rajat sharma\n",
            "  â†’ Score: 6.14/10\n",
            "\n",
            "[51/82] Evaluating: Venkatesh Dhangar \n",
            "  â†’ Score: 4.33/10\n",
            "\n",
            "[52/82] Evaluating: Aryan Jaiswal\n",
            "  â†’ Score: 5.42/10\n",
            "\n",
            "[53/82] Evaluating: Krishna Faujdar \n",
            "  â†’ Score: 4.55/10\n",
            "\n",
            "[54/82] Evaluating: Madhan S\n",
            "  â†’ Score: 4.11/10\n",
            "\n",
            "[55/82] Evaluating: Palshini B Limbani \n",
            "  â†’ Score: 4.38/10\n",
            "\n",
            "[56/82] Evaluating: Atharva Satish Attarde\n",
            "  â†’ Score: 5.89/10\n",
            "\n",
            "[57/82] Evaluating: Shreya Sanjay Khaire\n",
            "  â†’ Score: 6.03/10\n",
            "\n",
            "[58/82] Evaluating: Madhu R Koravanavar \n",
            "  â†’ Score: 4.27/10\n",
            "\n",
            "[59/82] Evaluating: Shaik Izhaar Ahmed\n",
            "  â†’ Score: 4.75/10\n",
            "\n",
            "[60/82] Evaluating: pratibha\n",
            "  â†’ Score: 3.96/10\n",
            "\n",
            "[61/82] Evaluating: S Nomtha Prakash \n",
            "  â†’ Score: 6.39/10\n",
            "\n",
            "[62/82] Evaluating: Sankeerth \n",
            "  â†’ Score: 6.23/10\n",
            "\n",
            "[63/82] Evaluating: P Dinesh karthik\n",
            "  â†’ Score: 5.1/10\n",
            "\n",
            "[64/82] Evaluating: Pranay Ch\n",
            "  â†’ Score: 6.41/10\n",
            "\n",
            "[65/82] Evaluating: Gokul Gorthi\n",
            "  â†’ Score: 3.63/10\n",
            "\n",
            "[66/82] Evaluating: Manas Deshmukh\n",
            "  â†’ Score: 5.49/10\n",
            "\n",
            "[67/82] Evaluating: V Shanmukha Sai \n",
            "  â†’ Score: 5.56/10\n",
            "\n",
            "[68/82] Evaluating: Jalluri Samtusta \n",
            "  â†’ Score: 6.05/10\n",
            "\n",
            "[69/82] Evaluating: Cheera Sai Sathwik \n",
            "  â†’ Score: 4.71/10\n",
            "\n",
            "[70/82] Evaluating: Kalyan Jakkoju \n",
            "  â†’ Score: 3.9/10\n",
            "\n",
            "[71/82] Evaluating: Pachabhatla Dhanush\n",
            "  â†’ Score: 4.93/10\n",
            "\n",
            "[72/82] Evaluating: Nayan kumar biradar\n",
            "  â†’ Score: 5.38/10\n",
            "\n",
            "[73/82] Evaluating: Manish B. Gaikwad \n",
            "  â†’ Score: 6.27/10\n",
            "\n",
            "[74/82] Evaluating: Mokshagna \n",
            "  â†’ Score: 3.91/10\n",
            "\n",
            "[75/82] Evaluating: Sai Aravind\n",
            "  â†’ Score: 5.05/10\n",
            "\n",
            "[76/82] Evaluating: Talla Vishnu Kranth Reddy\n",
            "  â†’ Score: 5.55/10\n",
            "\n",
            "[77/82] Evaluating: B.Dinesh\n",
            "  â†’ Score: 4.2/10\n",
            "\n",
            "[78/82] Evaluating: BC Jaswanth Reddy\n",
            "  â†’ Score: 2.48/10\n",
            "\n",
            "[79/82] Evaluating: S Siva Tejananda Reddy\n",
            "  â†’ Score: 6.11/10\n",
            "\n",
            "[80/82] Evaluating: Smaran Reddy\n",
            "  â†’ Score: 5.28/10\n",
            "\n",
            "[81/82] Evaluating: Saksham kamble\n",
            "  â†’ Score: 6.41/10\n",
            "\n",
            "[82/82] Evaluating: K V Jaya Harsha\n",
            "  â†’ Score: 3.38/10\n",
            "\n",
            "============================================================\n",
            "âœ“ Evaluation completed for all 82 students!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_all_summaries(summaries_df, transcript, model):\n",
        "    \"\"\"\n",
        "    Evaluate all student summaries\n",
        "\n",
        "    Args:\n",
        "        summaries_df: DataFrame with student summaries\n",
        "        transcript: Lecture transcript\n",
        "        model: Sentence Transformer model\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Results with scores and explanations\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    total_students = len(summaries_df)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Starting evaluation of {total_students} student summaries...\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Identify the summary column (handle different possible names)\n",
        "    summary_col = None\n",
        "    for col in summaries_df.columns:\n",
        "        if 'summary' in col.lower():\n",
        "            summary_col = col\n",
        "            break\n",
        "\n",
        "    if summary_col is None:\n",
        "        print(\"âœ— Error: Could not find summary column!\")\n",
        "        return None\n",
        "\n",
        "    for idx, row in summaries_df.iterrows():\n",
        "        student_num = idx + 1\n",
        "        print(f\"[{student_num}/{total_students}] Evaluating: {row.get('Name of the Student', 'Unknown')}\")\n",
        "\n",
        "        # Get summary text\n",
        "        summary_text = row[summary_col]\n",
        "\n",
        "        # Evaluate\n",
        "        evaluation = evaluate_summary(summary_text, transcript, model)\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'Email Address': row.get('Email Address', ''),\n",
        "            'Name': row.get('Name of the Student', ''),\n",
        "            'Roll Number': row.get('Roll number', ''),\n",
        "            'Institute': row.get('Institute', ''),\n",
        "            'Summary': summary_text,\n",
        "            'Score': evaluation['score'],\n",
        "            'Coverage': evaluation['coverage'],\n",
        "            'Relevance': evaluation['relevance'],\n",
        "            'Clarity': evaluation['clarity'],\n",
        "            'Coherence': evaluation['coherence'],\n",
        "            'Grammar': evaluation['grammar'],\n",
        "            'Explanation': evaluation['explanation']\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        print(f\"  â†’ Score: {evaluation['score']}/10\")\n",
        "        print()\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"âœ“ Evaluation completed for all {total_students} students!\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Run evaluation\n",
        "if transcript and summaries_df is not None:\n",
        "    results_df = evaluate_all_summaries(summaries_df, transcript, model)\n",
        "else:\n",
        "    print(\"âœ— Cannot proceed: Files not loaded properly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7661d5b7",
      "metadata": {
        "id": "7661d5b7"
      },
      "source": [
        "## Step 7: Display Results\n",
        "Showing evaluation statistics and top results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb152885",
      "metadata": {
        "id": "fb152885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "                         EVALUATION STATISTICS\n",
            "================================================================================\n",
            "\n",
            "Total Students Evaluated: 82\n",
            "Average Score: 5.16/10\n",
            "Highest Score: 7.09/10\n",
            "Lowest Score: 0.00/10\n",
            "Standard Deviation: 1.12\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Average Scores by Criterion:\n",
            "--------------------------------------------------------------------------------\n",
            "  Coverage (0-2):   0.69\n",
            "  Relevance (0-2):  0.40\n",
            "  Clarity (0-2):    1.21\n",
            "  Coherence (0-2):  1.07\n",
            "  Grammar (0-2):    1.79\n",
            "\n",
            "================================================================================\n",
            "                              TOP 5 RESULTS\n",
            "================================================================================\n",
            "\n",
            "Rank 1:\n",
            "  Name: Aryan Anilkumar Talikoti \n",
            "  Roll: 23bcs018\n",
            "  Score: 7.09/10\n",
            "  Feedback: Good coverage but misses some important details. lacks relevance to main lecture themes. Well-written with clear language and good grammar.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 2:\n",
            "  Name: M Jagadeeswar Reddy\n",
            "  Roll: 23bds033\n",
            "  Score: 6.59/10\n",
            "  Feedback: Excellent coverage of key lecture points. lacks relevance to main lecture themes. Well-written with clear language and good grammar.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 3:\n",
            "  Name: Deepak sharma\n",
            "  Roll: cs22b1024\n",
            "  Score: 6.57/10\n",
            "  Feedback: Good coverage but misses some important details. lacks relevance to main lecture themes. Adequate writing quality with minor issues.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 4:\n",
            "  Name: Amritanshu Aditya\n",
            "  Roll: 23BCS013\n",
            "  Score: 6.44/10\n",
            "  Feedback: Limited coverage of key lecture content. lacks relevance to main lecture themes. Well-written with clear language and good grammar.\n",
            "--------------------------------------------------------------------------------\n",
            "Rank 5:\n",
            "  Name: Pranay Ch\n",
            "  Roll: 22bcs088\n",
            "  Score: 6.41/10\n",
            "  Feedback: Good coverage but misses some important details. lacks relevance to main lecture themes. Adequate writing quality with minor issues.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "âœ“ Results preview complete!\n"
          ]
        }
      ],
      "source": [
        "if results_df is not None:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" \"*25 + \"EVALUATION STATISTICS\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(f\"Total Students Evaluated: {len(results_df)}\")\n",
        "    print(f\"Average Score: {results_df['Score'].mean():.2f}/10\")\n",
        "    print(f\"Highest Score: {results_df['Score'].max():.2f}/10\")\n",
        "    print(f\"Lowest Score: {results_df['Score'].min():.2f}/10\")\n",
        "    print(f\"Standard Deviation: {results_df['Score'].std():.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"Average Scores by Criterion:\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"  Coverage (0-2):   {results_df['Coverage'].mean():.2f}\")\n",
        "    print(f\"  Relevance (0-2):  {results_df['Relevance'].mean():.2f}\")\n",
        "    print(f\"  Clarity (0-2):    {results_df['Clarity'].mean():.2f}\")\n",
        "    print(f\"  Coherence (0-2):  {results_df['Coherence'].mean():.2f}\")\n",
        "    print(f\"  Grammar (0-2):    {results_df['Grammar'].mean():.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" \"*30 + \"TOP 5 RESULTS\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    top_5 = results_df.nlargest(5, 'Score')[['Name', 'Roll Number', 'Score', 'Explanation']]\n",
        "\n",
        "    for idx, row in top_5.iterrows():\n",
        "        print(f\"Rank {list(top_5.index).index(idx) + 1}:\")\n",
        "        print(f\"  Name: {row['Name']}\")\n",
        "        print(f\"  Roll: {row['Roll Number']}\")\n",
        "        print(f\"  Score: {row['Score']}/10\")\n",
        "        print(f\"  Feedback: {row['Explanation']}\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "    print(\"\\nâœ“ Results preview complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5e8d21",
      "metadata": {
        "id": "7d5e8d21"
      },
      "source": [
        "## Step 8: Export Results to Excel\n",
        "Saving graded results with formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b18dbb",
      "metadata": {
        "id": "73b18dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "                         EXPORT SUCCESSFUL\n",
            "================================================================================\n",
            "\n",
            "âœ“ Results exported to: Graded_results_23bds024.xlsx\n",
            "  Location: c:\\Users\\RAVIPRAKASH\\Downloads\\Agentic AI Quiz\\Graded_results_23bds024.xlsx\n",
            "  Total records: 82\n",
            "  Sheets created: 'Graded Results', 'Statistics'\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def export_results(results_df, roll_number='23bds024'):\n",
        "    \"\"\"\n",
        "    Export evaluation results to Excel file\n",
        "\n",
        "    Args:\n",
        "        results_df: DataFrame with evaluation results\n",
        "        roll_number: Student roll number for filename\n",
        "    \"\"\"\n",
        "    output_filename = f'Graded_results_{roll_number}.xlsx'\n",
        "\n",
        "    try:\n",
        "        # Sort by score (highest first)\n",
        "        results_sorted = results_df.sort_values('Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "        # Create Excel writer\n",
        "        with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
        "            # Write main results\n",
        "            results_sorted.to_excel(writer, sheet_name='Graded Results', index=False)\n",
        "\n",
        "            # Get workbook and worksheet\n",
        "            workbook = writer.book\n",
        "            worksheet = writer.sheets['Graded Results']\n",
        "\n",
        "            # Adjust column widths\n",
        "            column_widths = {\n",
        "                'A': 30,  # Email\n",
        "                'B': 25,  # Name\n",
        "                'C': 15,  # Roll Number\n",
        "                'D': 20,  # Institute\n",
        "                'E': 50,  # Summary\n",
        "                'F': 10,  # Score\n",
        "                'G': 12,  # Coverage\n",
        "                'H': 12,  # Relevance\n",
        "                'I': 12,  # Clarity\n",
        "                'J': 12,  # Coherence\n",
        "                'K': 12,  # Grammar\n",
        "                'L': 60   # Explanation\n",
        "            }\n",
        "\n",
        "            for col, width in column_widths.items():\n",
        "                worksheet.column_dimensions[col].width = width\n",
        "\n",
        "            # Create summary statistics sheet\n",
        "            stats_data = {\n",
        "                'Metric': ['Total Students', 'Average Score', 'Highest Score', 'Lowest Score',\n",
        "                          'Standard Deviation', 'Avg Coverage', 'Avg Relevance', 'Avg Clarity',\n",
        "                          'Avg Coherence', 'Avg Grammar'],\n",
        "                'Value': [\n",
        "                    len(results_sorted),\n",
        "                    f\"{results_sorted['Score'].mean():.2f}/10\",\n",
        "                    f\"{results_sorted['Score'].max():.2f}/10\",\n",
        "                    f\"{results_sorted['Score'].min():.2f}/10\",\n",
        "                    f\"{results_sorted['Score'].std():.2f}\",\n",
        "                    f\"{results_sorted['Coverage'].mean():.2f}/2\",\n",
        "                    f\"{results_sorted['Relevance'].mean():.2f}/2\",\n",
        "                    f\"{results_sorted['Clarity'].mean():.2f}/2\",\n",
        "                    f\"{results_sorted['Coherence'].mean():.2f}/2\",\n",
        "                    f\"{results_sorted['Grammar'].mean():.2f}/2\"\n",
        "                ]\n",
        "            }\n",
        "            stats_df = pd.DataFrame(stats_data)\n",
        "            stats_df.to_excel(writer, sheet_name='Statistics', index=False)\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\" \"*25 + \"EXPORT SUCCESSFUL\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"\\nâœ“ Results exported to: {output_filename}\")\n",
        "        print(f\"  Location: {os.path.abspath(output_filename)}\")\n",
        "        print(f\"  Total records: {len(results_sorted)}\")\n",
        "        print(f\"  Sheets created: 'Graded Results', 'Statistics'\")\n",
        "        print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "        return output_filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâœ— Error exporting results: {e}\")\n",
        "        return None\n",
        "\n",
        "# Export results\n",
        "if results_df is not None:\n",
        "    output_file = export_results(results_df, roll_number='23bds024')\n",
        "else:\n",
        "    print(\"âœ— No results to export\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a8c57f5",
      "metadata": {
        "id": "3a8c57f5"
      },
      "source": [
        "## Step 9: Final Summary\n",
        "System execution summary and demo readiness check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a119a730",
      "metadata": {
        "id": "a119a730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "################################################################################\n",
            "#                                                                              #\n",
            "#                    AI SUMMARY EVALUATOR - EXECUTION COMPLETE                 #\n",
            "#                                                                              #\n",
            "################################################################################\n",
            "\n",
            "âœ“ System successfully completed all tasks:\n",
            "\n",
            "  1. âœ“ Loaded lecture transcript from transcript.docx\n",
            "  2. âœ“ Loaded student summaries from summary.xlsx\n",
            "  3. âœ“ Initialized AI model (Sentence Transformer)\n",
            "  4. âœ“ Evaluated all summaries on 5 criteria:\n",
            "       â€¢ Coverage - Key content capture\n",
            "       â€¢ Relevance - Alignment with transcript\n",
            "       â€¢ Clarity - Readability and flow\n",
            "       â€¢ Coherence - Logical structure\n",
            "       â€¢ Grammar - Language correctness\n",
            "  5. âœ“ Generated scores (0-10) with explanations\n",
            "  6. âœ“ Created Excel file: Graded_results_22bds053.xlsx\n",
            "  7. âœ“ Displayed top 5 results for inspection\n",
            "\n",
            "ðŸ“Š Evaluation Summary:\n",
            "   â€¢ Students evaluated: 82\n",
            "   â€¢ Average score: 5.16/10\n",
            "   â€¢ Score range: 0.00 - 7.09\n",
            "\n",
            "################################################################################\n",
            "#                         SYSTEM READY FOR DEMO VIDEO                          #\n",
            "################################################################################\n",
            "\n",
            "ðŸ“¹ Demo Checklist:\n",
            "   âœ“ Input files loaded and processed\n",
            "   âœ“ AI evaluation pipeline functional\n",
            "   âœ“ Results generated with scores and feedback\n",
            "   âœ“ Excel output file created in working directory\n",
            "   âœ“ Top results displayed for verification\n",
            "\n",
            "   Ready to record 5-minute demonstration video!\n",
            "\n",
            "================================================================================\n",
            "To re-run: Click 'Run All' or restart kernel and execute all cells\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"#\" + \" \"*78 + \"#\")\n",
        "print(\"#\" + \" \"*20 + \"AI SUMMARY EVALUATOR - EXECUTION COMPLETE\" + \" \"*17 + \"#\")\n",
        "print(\"#\" + \" \"*78 + \"#\")\n",
        "print(\"#\"*80 + \"\\n\")\n",
        "\n",
        "print(\"âœ“ System successfully completed all tasks:\")\n",
        "print(\"\\n  1. âœ“ Loaded lecture transcript from transcript.docx\")\n",
        "print(\"  2. âœ“ Loaded student summaries from summary.xlsx\")\n",
        "print(\"  3. âœ“ Initialized AI model (Sentence Transformer)\")\n",
        "print(\"  4. âœ“ Evaluated all summaries on 5 criteria:\")\n",
        "print(\"       â€¢ Coverage - Key content capture\")\n",
        "print(\"       â€¢ Relevance - Alignment with transcript\")\n",
        "print(\"       â€¢ Clarity - Readability and flow\")\n",
        "print(\"       â€¢ Coherence - Logical structure\")\n",
        "print(\"       â€¢ Grammar - Language correctness\")\n",
        "print(\"  5. âœ“ Generated scores (0-10) with explanations\")\n",
        "print(\"  6. âœ“ Created Excel file: Graded_results_22bds053.xlsx\")\n",
        "print(\"  7. âœ“ Displayed top 5 results for inspection\\n\")\n",
        "\n",
        "if results_df is not None:\n",
        "    print(f\"ðŸ“Š Evaluation Summary:\")\n",
        "    print(f\"   â€¢ Students evaluated: {len(results_df)}\")\n",
        "    print(f\"   â€¢ Average score: {results_df['Score'].mean():.2f}/10\")\n",
        "    print(f\"   â€¢ Score range: {results_df['Score'].min():.2f} - {results_df['Score'].max():.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"#\" + \" \"*25 + \"SYSTEM READY FOR DEMO VIDEO\" + \" \"*26 + \"#\")\n",
        "print(\"#\"*80 + \"\\n\")\n",
        "\n",
        "print(\"ðŸ“¹ Demo Checklist:\")\n",
        "print(\"   âœ“ Input files loaded and processed\")\n",
        "print(\"   âœ“ AI evaluation pipeline functional\")\n",
        "print(\"   âœ“ Results generated with scores and feedback\")\n",
        "print(\"   âœ“ Excel output file created in working directory\")\n",
        "print(\"   âœ“ Top results displayed for verification\")\n",
        "print(\"\\n   Ready to record 5-minute demonstration video!\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"To re-run: Click 'Run All' or restart kernel and execute all cells\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
